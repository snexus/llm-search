cache_folder: /path/to/cache/folder ## specify a cache folder for embeddings models, huggingface and sentence transformers

embeddings:
  # ** Attention ** - `embedding_path` should be uniquer per configuration file.
  embeddings_path: /path/to/embedding/folder ## specify a folder where embeddings will be saved.
  
  embedding_model: # Optional embedding model specification, default is e5-large-v2. Swap to a smaller model if out of CUDA memory
    # Supported types: "huggingface", "instruct", "openai"
    type: sentence_transformer # other supported types - "huggingface" and "instruct"
    model_name: "vsolatorio/GIST-Embedding-v0"
  
  splade_config: # Optional batch size of sparse embeddings. Reduce if getting out-of-memory errors on CUDA.
    n_batch: 5
  
  chunk_sizes: # Specify one more chunk size to split (querying multi-chunk results will be slower)
    - 1024

  document_settings:

  # Can specify multiple documents collections and filter by label

  - doc_path: /path/to/documents ## specify the docs folder
    exclude_paths: # Optional paths to exclude
      - /path/to/documents/subfolder1
      - /path/to/documents/subfolder2
    scan_extensions: # specifies files extensions to scan recursively in `doc_path`. 
      - pdf
      - md
    additional_parser_settings: # Optional section, don't have to include
      md: 
        skip_first: True  # Skip first section which often contains metadata
        merge_sections: False # Merge # headings if possible, can be turned on and off depending on document stucture
        remove_images: True # Remove image links
    
    # Optional setting
    # For azuredoc support - pip install "pyllmsearch[azureparser]"
    pdf_table_parser: gmft # azuredoc

    # Optional setting
    pdf_image_parser:
        image_parser: gemini-1.5-pro # gemini-1.5-flash
        system_instructions: |
            You are an research assistant. You analyze the image to extract detailed information. Response must be a Markdown string in the follwing format:
            - First line is a heading with image caption, starting with '# '
            - Second line is empty
            - From the third line on - detailed data points and related metadata, extracted from the image, in Markdown format. Don't use Markdown tables.

    
    passage_prefix: "passage: " # Often, specific prefix needs to be included in the source text, for embedding models to work properly
    label: "documment-collection-1" # Add a label to the current collection
  
  - doc_path: /another/path/to/documents ## specify the docs folder
    scan_extensions: # specifies files extensions to scan recursively in `doc_path`. 
      - md
    
    passage_prefix: "passage: " # Often, specific prefix needs to be included in the source text, for embedding models to work properly
    label: "documment-collection-2" # Add a label to the current collection

semantic_search:
  search_type: similarity # Currently, only similarity is supported
  replace_output_path: # Can specify list of search/replace settings
    - substring_search: "/storage/llm/docs/" ## Specifies substring to replace  in the output path of the document
      substring_replace: "obsidian://open?vault=knowledge-base&file=" ## Replaces with this string

  append_suffix: # Specifies additional template to append to an output path, useful for deep linking
    append_template: "#page={page}" # For example will append a page from metadata of the document parser

  # Will ensure that context provided to LLM is less than max_char_size. Useful for locally hosted models and limited hardware. 
  # Reduce if out of CUDA memory.
  max_char_size: 16384 # Reduce if necessary for locally hosted LLMs

  # Maximum number of text chunks to retrive for dense and sparse embeddings
  # Total number of chunks is max_k * 2
  max_k: 25
  
  query_prefix: "query: " # Often queries have to be prefixed for embedding models, such as e5

  score_cutoff: -3.0 # Optional reranker score cutoff. Documents below this score will be excluded from the returned document list

  hyde:
    enabled: False
  
  multiquery: 
    enabled: False
  
  reranker:
    enabled: True
    model: "bge" # for `BAAI/bge-reranker-base` or "marco" for cross-encoder/ms-marco-MiniLM-L-6-v2
  
  # Optionally enable conversation history settings (default False)
  conversation_history_settings:
    enabled: True
    max_history_length: 3
    rewrite_query: True

  

persist_response_db_path:  "/path/to/responses.db" # optional sqlite database filename. Allows to save responses offlien to sqlite, for future analysis.


